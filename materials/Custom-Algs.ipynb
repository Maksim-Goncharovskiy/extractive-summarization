{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d2b674",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Алгоритмы составления экстрактивной суммаризации</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4b63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdc568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Максат\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Максат\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Максат\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Максат\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468a11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymorphy3\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350f253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c2fe4",
   "metadata": {},
   "source": [
    "## 1. Алгоритм на основе вхождения общих слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e02efa",
   "metadata": {},
   "source": [
    "Шаги алгоритма:\n",
    "1. Разбиение текста на предложения.\n",
    "2. Каждое предложение разбивается на токены(отдельные слова), слова лемматизируются\n",
    "3. Считается функция схожести каждой пары предложений(отношение числа общих слов к их суммарной длине) -> матрица схожести\n",
    "4. Отсеиваем предложения, не имеющие общих слов ни с какими другими.\n",
    "5. На основе матрицы схожести строим граф: вершины - сами предложения, рёбра - наличие схожести между ними.\n",
    "6. Считаем значимость каждого предложения и составляем экстрактивную суммаризацию из N наиболее значимых предложений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930b2ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:39: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Максат\\AppData\\Local\\Temp\\ipykernel_7080\\2641881868.py:39: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = text = re.sub('[^a-zA-Zа-яА-Я0-9.\\s]', '', text)\n",
      "C:\\Users\\Максат\\AppData\\Local\\Temp\\ipykernel_7080\\2641881868.py:40: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  text = re.sub('\\.', ' . ', text)\n"
     ]
    }
   ],
   "source": [
    "class CommonWordsSummarizer():\n",
    "    '''\n",
    "    Класс, реализующий алгоритм составления экстрактивной суммаризации на основе вхождения общих слов.\n",
    "    '''\n",
    "    def __init__(self, language=\"english\", stop_words=False, make_lemmatization=True):\n",
    "        \"\"\"\n",
    "        Инициализация объекта класса суммаризатора.\n",
    "        \n",
    "        Параметры:\n",
    "            language (str): Язык текстов - 'russian' или 'english'\n",
    "            stop_words (boolean): Удалять или нет стоп-слова\n",
    "            make_lemmatization (boolean): Проводить или нет лемматизацию слов\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        if (stop_words):  \n",
    "            self.stop_words = set(stopwords.words(self.language))\n",
    "        else:\n",
    "            self.stop_words = None\n",
    "        \n",
    "        self.make_lemmatization = make_lemmatization\n",
    "        self.lemmatizer_ru = pymorphy3.MorphAnalyzer()\n",
    "        self.lemmatizer_en = WordNetLemmatizer()\n",
    "        \n",
    "        \n",
    "    def __preprocessor(self, text):\n",
    "        \"\"\"\n",
    "        Предобработка текста: \n",
    "        1) удаление ссылок, тегов, номеров\n",
    "        2) приведение к нижнему регистру\n",
    "        3) удаление стоп-слов\n",
    "        4) лемматизация\n",
    "        \n",
    "        Возвращает:\n",
    "            строку, полностью предобработанную. \n",
    "        \"\"\"\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text = re.sub('[^a-zA-Zа-яА-Я0-9.\\s]', '', text)\n",
    "        text = re.sub('\\.', ' . ', text)\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "        if (self.stop_words):\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in self.stop_words])\n",
    "        \n",
    "        if (self.make_lemmatization):\n",
    "            if (self.language==\"english\"):\n",
    "                text = ' '.join([self.lemmatizer_en.lemmatize(word, self.__get_wordnet_pos(word)) for word in text.split()])\n",
    "            else:\n",
    "                text = ' '.join([self.lemmatizer_ru.parse(word)[0].normal_form for word in text.split()])\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def __get_wordnet_pos(self, word):\n",
    "        \"\"\"Функция для преобразования POS-тегов из NLTK в формат, подходящий для WordNetLemmatizer\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\n",
    "            'J': wordnet.ADJ,\n",
    "            'N': wordnet.NOUN,\n",
    "            'V': wordnet.VERB,\n",
    "            'R': wordnet.ADV\n",
    "        }\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    def __make_similarity_matrix(self, sentences):\n",
    "        \"\"\"\n",
    "        Функция создания матрицы схожести между всеми парами предложений текста.\n",
    "        Параметры:\n",
    "            sentences: массив предложений(полностью предобработанных)\n",
    "        Возвращает:\n",
    "            матрица схожести размера NxN, N - количество предложений\n",
    "        \"\"\"\n",
    "        similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if (i != j):\n",
    "                    similarity_matrix[i][j] = self.__get_similarity(sentences[i], sentences[j])\n",
    "        return similarity_matrix\n",
    "    \n",
    "    \n",
    "    def __get_similarity(self, sent_1, sent_2):\n",
    "        \"\"\"\n",
    "        Функция, оценивающая схожесть двух предложений на основании количества общих слов.\n",
    "        Параметры:\n",
    "            sent_1, sent_2 - две строки(предложения)\n",
    "        Возвращает: \n",
    "            - одно число, схожесть\n",
    "        \"\"\"\n",
    "        set_1 = set(sent_1.split())\n",
    "        set_2 = set(sent_2.split())\n",
    "        \n",
    "        return len(set_1 & set_2) / (len(sent_1.split() + sent_2.split()))\n",
    "    \n",
    "    \n",
    "    def summarize(self, text, N=3):\n",
    "        \"\"\"\n",
    "        Функция, возвращающая экстрактивную суммаризацию текста.\n",
    "        Параметры:\n",
    "            text - строка, м.б. непредобработанная.\n",
    "            N - количество предложений, которые войдут в суммаризацию.\n",
    "        Возвращает:\n",
    "            строку, содержащую N предложений - экстрактивная суммаризация\n",
    "        \"\"\"\n",
    "        \n",
    "        # Предобработка текста\n",
    "        preprocessed_text = self.__preprocessor(text)\n",
    "        # Разбиение текста на предложения\n",
    "        sentences = sent_tokenize(preprocessed_text)\n",
    "        \n",
    "        # Удаление точки и пробела в конце каждого предложения\n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = sentences[i][0:len(sentences[i])-2]\n",
    "        \n",
    "        # Нахождение матрицы схожести\n",
    "        similarity_matrix = self.__make_similarity_matrix(sentences)\n",
    "        \n",
    "        # Создаем граф, вершины - предложения, рёбра - схожесть\n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "        # Вычисляем значимость каждого предложения\n",
    "        scores = nx.pagerank(graph)\n",
    "        \n",
    "        scores = list(scores.items())\n",
    "        # Сортируем предложения по их значимости\n",
    "        scores.sort(reverse=True, key=lambda x: x[1])\n",
    "        \n",
    "        # Отбираем N самых значимых предложений и сортируем их по номерам(порядку в исходном тексте)\n",
    "        best_N_scores = scores[0:N]\n",
    "        best_N_scores.sort(key=lambda x: x[0])\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        summarization = ' '.join([sentences[best_N_scores[i][0]] for i in range(N)])\n",
    "        \n",
    "        return summarization\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e4bee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Maria Sharapova has basically no friends as tennis players on the WTA Tour. I have not a lot of friends away from the courts.' Is it different on the men's tour than the women's tour? 'No, not at all. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en = CommonWordsSummarizer(stop_words=True,language='english')\n",
    "text_en = \"Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here. When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well. Uhm, I'm not really friendly or close to many players. I have not a lot of friends away from the courts.' When she said she is not really close to a lot of players, is that something strategic that she is doing? Is it different on the men's tour than the women's tour? 'No, not at all. I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players. I think every person has different interests. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life. I think everyone just thinks because we're tennis players we should be the greatest of friends. But ultimately tennis is just a very small part of what we do. There are so many other things that we're interested in, that we do.'\"\n",
    "model_en.summarize(text_en, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69937508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ПФЛ выступила с заявлением по поводу ситуации с завершением сезона-2019/20. Ранее большинство клубов в ФНЛ также проголосовали за досрочное завершение сезона.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ru = CommonWordsSummarizer(stop_words=True,language='russian', make_lemmatization=True)\n",
    "text_ru = \"ПФЛ выступила с заявлением по поводу ситуации с завершением сезона-2019/20. Лига отмечает, что доиграть сезон, прерванный из-за пандемии короновируса, проблематично. Клубы в текущий и ближайший период не могут проводить тренировочные мероприятия, существует неопределённость по срокам возобновления соревнований, кроме того, у клубов возникнут дополнительные организационные и финансовые издержки в случае продления паузы. Клубы ПФЛ в ходе видеоконференци в апреле отметили, что для проведения оставшихся игр им потребуется период в 1,5 месяца, а также не меньше трех недель на тренировки для набора формы. Совет и администрация ПФЛ призвали объявить сезон завершённым и предоставить право выхода в ФНЛ тем клубам, которые имеют лучшие спортивные показатели и соответствуют необходимым требованиям. Ранее большинство клубов в ФНЛ также проголосовали за досрочное завершение сезона.\"\n",
    "model_ru.summarize(text_ru, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72996e",
   "metadata": {},
   "source": [
    "## 2. Алгоритм на основе обученных векторных представлений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "124794bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg.lapack' (F:\\Downloads\\Anaconda\\Lib\\site-packages\\scipy\\linalg\\lapack.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#from gensim.models import Word2Vec\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastText\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "File \u001b[1;32mF:\\Downloads\\Anaconda\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Downloads\\Anaconda\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Downloads\\Anaconda\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mF:\\Downloads\\Anaconda\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mF:\\Downloads\\Anaconda\\Lib\\site-packages\\gensim\\matutils.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n\u001b[0;32m     25\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg.lapack' (F:\\Downloads\\Anaconda\\Lib\\site-packages\\scipy\\linalg\\lapack.py)"
     ]
    }
   ],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd95e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilaritySummarizer():\n",
    "    def __init__(self, model='Word2Vec', language=\"english\", stop_words=False, make_lemmatization=True):\n",
    "        \"\"\"\n",
    "        Инициализация объекта класса суммаризатора.\n",
    "        \n",
    "        Параметры:\n",
    "            model (str): Используемая модель векторизации слов - 'Word2Vec' или 'FastText'\n",
    "            language (str): Язык текстов - 'russian' или 'english'\n",
    "            stop_words (boolean): Удалять или нет стоп-слова\n",
    "            make_lemmatization (boolean): Проводить или нет лемматизацию слов\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.vectorizer = None\n",
    "        \n",
    "        self.language = language\n",
    "        if (stop_words):  \n",
    "            self.stop_words = set(stopwords.words(self.language))\n",
    "        else:\n",
    "            self.stop_words = None\n",
    "        \n",
    "        self.make_lemmatization = make_lemmatization\n",
    "        self.lemmatizer_ru = pymorphy3.MorphAnalyzer()\n",
    "        self.lemmatizer_en = WordNetLemmatizer()\n",
    "        \n",
    "    def __get_wordnet_pos(self, word):\n",
    "        \"\"\"Функция для преобразования POS-тегов из NLTK в формат, подходящий для WordNetLemmatizer\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\n",
    "            'J': wordnet.ADJ,\n",
    "            'N': wordnet.NOUN,\n",
    "            'V': wordnet.VERB,\n",
    "            'R': wordnet.ADV\n",
    "        }\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    def __preprocessor(self, text):\n",
    "        \"\"\"\n",
    "        Предобработка текста: \n",
    "        1) удаление ссылок, тегов, номеров\n",
    "        2) приведение к нижнему регистру\n",
    "        3) удаление стоп-слов\n",
    "        4) лемматизация\n",
    "        \n",
    "        Возвращает:\n",
    "            строку, полностью предобработанную. \n",
    "        \"\"\"\n",
    "        \n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text = re.sub('[^a-zA-Zа-яА-Я0-9.\\s]', '', text)\n",
    "        text = re.sub('\\.', ' . ', text)\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "        if (self.stop_words):\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in self.stop_words])\n",
    "        \n",
    "        if (self.make_lemmatization):\n",
    "            if (self.language==\"english\"):\n",
    "                text = ' '.join([self.lemmatizer_en.lemmatize(word, self.__get_wordnet_pos(word)) for word in text.split()])\n",
    "            else:\n",
    "                text = ' '.join([self.lemmatizer_ru.parse(word)[0].normal_form for word in text.split()])\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        Функция обучения модели векторизации слов \n",
    "        Параметры:\n",
    "            texts (array) - массив текстов(строк, м.б. непредобработанными)\n",
    "        \"\"\"\n",
    "        # предобрабатываем каждый текст\n",
    "        preprocessed_texts = []\n",
    "        for text in texts:\n",
    "            text = self.__preprocessor(text)\n",
    "            preprocessed_texts.append([word for word in text.split() if word != '.'])\n",
    "           \n",
    "        # обучаем на полученных словах модели векторизации\n",
    "        \n",
    "        if (self.model == \"Word2Vec\"):\n",
    "            self.vectorizer = Word2Vec(sentences=preprocessed_texts, min_count=5, vector_size=100)\n",
    "        elif (self.model == \"FastText\"):\n",
    "            self.vectorizer = FastText(sentences=preprocessed_texts, min_count=5, vector_size=100)\n",
    "    \n",
    "    \n",
    "    def __get_vec(self, word):\n",
    "        \"\"\"\n",
    "        Получения вектора слова. Если слово не знакомо -> возвращается нулевой вектор\n",
    "        \"\"\"\n",
    "        if word in self.vectorizer.wv:\n",
    "            return self.vectorizer.wv[word]\n",
    "        else:\n",
    "            return np.array([0]*100, dtype='float32')\n",
    "        \n",
    "        \n",
    "    def __get_similarity(self, sent_1, sent_2):\n",
    "        \"\"\"\n",
    "        Функция, оценивающая схожесть двух предложений на основании косинусного расстояния\n",
    "        Параметры:\n",
    "            sent_1, sent_2 - две строки(предложения)\n",
    "        Возвращает: \n",
    "            - одно число, косинусное расстояние\n",
    "        \"\"\"\n",
    "        # Инициализируем векторные представления предложений\n",
    "        first_vector = np.array([0]*100, dtype='float32')\n",
    "        second_vector = np.array([0]*100, dtype='float32')\n",
    "        \n",
    "        # Получим списки слов каждого предложения\n",
    "        words_1 = sent_1.split()\n",
    "        words_2 = sent_2.split()\n",
    "        \n",
    "        # Вектор предложения - сумма векторов его слов, деленная на количество слов\n",
    "        for word in words_1:\n",
    "            first_vector += self.__get_vec(word)\n",
    "        first_vector /= len(words_1)\n",
    "            \n",
    "        for word in words_2:\n",
    "            second_vector += self.__get_vec(word)\n",
    "        second_vector /= len(words_2)\n",
    "        \n",
    "        return cosine_similarity(first_vector.reshape((1, 100)), second_vector.reshape((1, 100)))[0][0]\n",
    "        \n",
    "    \n",
    "    def __make_similarity_matrix(self, sentences):\n",
    "        similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if (i != j):\n",
    "                    similarity_matrix[i][j] = self.__get_similarity(sentences[i], sentences[j])\n",
    "        return similarity_matrix\n",
    "    \n",
    "    \n",
    "    def summarize(self, text, N=3):\n",
    "        preprocessed_text = self.__preprocessor(text)\n",
    "        sentences = sent_tokenize(preprocessed_text)\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = sentences[i][0:len(sentences[i])-2]\n",
    "        \n",
    "        similarity_matrix = self.__make_similarity_matrix(sentences)\n",
    "        \n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(graph)\n",
    "        \n",
    "        scores = list(scores.items())\n",
    "        scores.sort(reverse=True, key=lambda x: x[1])\n",
    "        \n",
    "        best_N_scores = scores[0:N]\n",
    "        best_N_scores.sort(key=lambda x: x[0])\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        summarization = ' '.join([sentences[best_N_scores[i][0]] for i in range(N)])\n",
    "        \n",
    "        return summarization\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bfeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f2bff6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maria Sharapova has basically no friends as te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BASEL, Switzerland (AP), Roger Federer advance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roger Federer has revealed that organisers of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kei Nishikori will try to end his long losing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Federer, 37, first broke through on tour over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nadal has not played tennis since he was force...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tennis giveth, and tennis taketh away. The end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Federer won the Swiss Indoors last week by bea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text\n",
       "0  Maria Sharapova has basically no friends as te...\n",
       "1  BASEL, Switzerland (AP), Roger Federer advance...\n",
       "2  Roger Federer has revealed that organisers of ...\n",
       "3  Kei Nishikori will try to end his long losing ...\n",
       "4  Federer, 37, first broke through on tour over ...\n",
       "5  Nadal has not played tennis since he was force...\n",
       "6  Tennis giveth, and tennis taketh away. The end...\n",
       "7  Federer won the Swiss Indoors last week by bea..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tennis_articles_v4.csv')\n",
    "df = pd.DataFrame(df['article_text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d007f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CosineSimilaritySummarizer(stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f223d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df['article_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b672963f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Maria Sharapova has basically no friends as tennis players on the WTA Tour. 'No, not at all. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summarize(text_en, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cffae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
